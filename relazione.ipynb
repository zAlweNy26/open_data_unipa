{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><strong>Relazione di Open Data Management 2023-2024</strong></h1>\n",
    "<h3 align=\"center\"><strong><em>di Daniele Nicosia e Claudio Bellanti</em></strong></h3>\n",
    "<h5 align=\"center\"><em>Università degli Studi di Palermo - Facoltà di Informatica</em></h5>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "### [`1. - Traccia`](#1-traccia)\n",
    "### [`2. - Selezione dei dataset`](#2-selezione-dei-dataset)\n",
    "- ##### [`2.1 - Raccolta`](#21-raccolta)\n",
    "- ##### [`2.2 - Licenze`](#22-licenze)\n",
    "### [`3. - Elaborazione dei dataset`](#3-elaborazione-dei-dataset)\n",
    "- ##### [`3.1 - Pulizia e selezione dei dati rilevanti`](#31-pulizia-e-selezione-dei-dati-rilevanti)\n",
    "- ##### [`3.2 - Arricchimento`](#32-arricchimento)\n",
    "### [`4. - Finalizzazione dei dataset`](#4-finalizzazione-dei-dataset)\n",
    "- ##### [`4.1 - Ontologia`](#41-ontologia)\n",
    "- ##### [`4.2 - Conversione in RDF`](#42-conversione-in-rdf)\n",
    "### [`5. - Visualizzazione dati`](#5-visualizzazione-dati)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1. - Traccia`**\n",
    "\n",
    "Utilizzando il linguaggio di programmazione Python, per lo sviluppo del progetto si devono innanzitutto rispettare i seguenti passi:\n",
    "\n",
    "- _Selezione dati_\n",
    "- _Elaborazione dati (data cleaning, definizione struttura omogenea)_\n",
    "- _Open Linked Data (creazione di uno strato semantico, ontologie, interlinking)_\n",
    "\n",
    "L'obiettivo della suddetta relazione è lo sviluppo di ???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2. - Selezione dei dataset`**\n",
    "\n",
    "#### **`2.1 - Raccolta`**\n",
    "\n",
    "Per lo sviluppo del progetto sono stati selezionati dei dataset i cui dati rappresentino gli eventi che si sono verificati nel Comune di Milano nel periodo 2014-2021. I dataset selezionati sono i seguenti:\n",
    "\n",
    "- [**`Parco veicoli circolanti per classe di inquinamento 2007-2022`**](https://dati.comune.milano.it/dataset/ds530-emissioni-inquinanti-standard-eu-2007-avanti)\n",
    "- [**`Ricoveri ordinari apparato respiratorio 2007-2021`**](https://dati.comune.milano.it/dataset/ds1053_ricoveri-ordinari-apparato-respiratorio)\n",
    "- [**`Stazioni di monitoraggio inquinanti`**](https://dati.comune.milano.it/dataset/ds484_stazioni_di_monitoraggio_inquinanti_atmosferici_dellarpa_sit)\n",
    "- [**`Ricostruzione della popolazione 2002-2019`**](https://demo.istat.it/app/?i=RIC&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2002 al 2019.\n",
    "- [**`Popolazione residente 2019-2023`**](https://demo.istat.it/app/?i=POS&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2020 al 2022.\n",
    "- [**`Standard di emissioni CO europei`**](https://en.wikipedia.org/wiki/European_emission_standards#Toxic_emission:_stages_and_legal_framework)\n",
    "  - Per questo dataset è stato necessario effettuare uno scraping del sito. Il dataset risultante contiene gli standard di emissioni CO europei dal 1992 al 2021.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`2.2 - Licenze`**\n",
    "\n",
    "Le licenze dei dataset selezionati sono le seguenti:\n",
    "\n",
    "- **`Parco veicoli circolanti per classe di inquinamento 2007-2022`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricoveri ordinari apparato respiratorio 2007-2021`**: Creative Commons Attribuzione-Condividi allo stesso modo 3.0 Italia ([CC BY-SA 3.0 IT](https://creativecommons.org/licenses/by-sa/3.0/it/))\n",
    "- **`Stazioni di monitoraggio inquinanti`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricostruzione della popolazione 2002-2019`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://www.istat.it/it/note-legali))\n",
    "- **`Popolazione residente 2019-2023`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Standard di emissioni CO europei`**: Creative Commons Attribuzione-ShareAlike 4.0 Internazionale ([CC BY-SA 4.0](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License))\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3. - Elaborazione dei dataset`**\n",
    "\n",
    "#### **`3.1 - Pulizia e selezione dei dati rilevanti`**\n",
    "\n",
    "Il primo passo effettuato è stato unire i diversi dataset della popolazione residente nel Comune di Milano, in un unico dataset.\n",
    "\n",
    "Si è notato che i dataset puri non erano tutti adattati allo stesso modo, infatti, si sono riscontrate due problematiche:\n",
    "\n",
    "- Encoding dei dataset differenti, a causa delle loro diversi origini\n",
    "- Errori vari dovuti a `;` o `,` non correttamente inseriti\n",
    "\n",
    "Inoltre, per il dataset delle emissioni è stato necessario effettuare uno scraping poichè non era disponibile in formato CSV. Per effettuare lo scraping è stato utilizzato il linguaggio di programmazione Python e la libreria `BeautifulSoup`. Lo script utilizzato è accessibile qui: [**`scraping.ipynb`**](dataset/emissioni/scraping.ipynb).\n",
    "\n",
    "##### *`Preparazione ambiente di sviluppo`*\n",
    "\n",
    "Prima di cominciare ad analizzare i dati dobbiamo importare le librerie necessarie al nostro scopo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset rilevazione della qualità dell'aria`*\n",
    "\n",
    "In questo estratto di codice, ci siamo concentrati sull'elaborazione di un insieme di dati complesso riguardante la qualità dell'aria, raccolto nel corso di diversi anni. Questi dati provengono da molteplici stazioni che, è importante notare, hanno capacità di rilevamento diverse. Non tutte le stazioni sono equipaggiate per misurare ogni tipo di inquinante, il che introduce una certa variabilità nei dati che stiamo trattando.\n",
    "\n",
    "Il primo passo significativo nel nostro processo è stato unire questi diversi file di dati, ognuno contenente frammenti del quadro generale della qualità dell'aria, in un unico, coerente DataFrame. Questo non solo ha semplificato le fasi successive dell'analisi, ma ha anche assicurato che ogni pezzo di informazione fosse contestualizzato all'interno del dataset più ampio.\n",
    "\n",
    "Una volta consolidati i dati, abbiamo affrontato la questione della uniformità temporale, assicurandoci che ogni data fosse in un formato standard e ordinando poi il dataset in ordine cronologico. Questo ordine temporale è essenziale per qualsiasi analisi storica o trend che potrebbe emergere da questi dati.\n",
    "\n",
    "Il problema successivo che abbiamo affrontato è stato quello dei valori mancanti, un'incidenza comune nei grandi set di dati, specialmente quando si tratta di rilevamenti ambientali su più anni. La strategia adottata qui è stata calcolare la mediana dei valori per ogni inquinante per ogni anno, una tecnica scelta per la sua resistenza agli outlier, ovvero quei valori che differiscono significativamente dalla norma e che potrebbero distorcere i risultati se non trattati correttamente.\n",
    "\n",
    "Utilizzando le mediane annuali, abbiamo potuto imputare valori ragionevoli nei casi in cui i dati erano mancanti, mantenendo l'integrità generale dei dati senza concedere un peso eccessivo a misurazioni anomale o inaccurate. Questo passaggio è essenziale per mantenere la validità delle nostre conclusioni finali.\n",
    "\n",
    "Dopo aver riempito questi vuoti, il nostro set di dati era quasi pronto per essere utilizzato in analisi future. Abbiamo rimosso alcune colonne superflue per rendere il dataset più snello e abbiamo gestito eventuali valori mancanti residui, assicurandoci che fossero chiaramente indicati.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('./data/raw/rilevazione_qualità_aria/*.csv')\n",
    "\n",
    "# Carica tutti i file CSV in un dizionario di DataFrame, specificando il separatore corretto\n",
    "dfs = {fp: pd.read_csv(fp, sep=';') for fp in file_paths}\n",
    "\n",
    "# Concatena tutti i DataFrame in un unico DataFrame\n",
    "air_quality = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Converti la colonna 'data' in datetime se non lo è già\n",
    "air_quality['data'] = pd.to_datetime(air_quality['data'])\n",
    "\n",
    "# Ordina il DataFrame in base alla colonna 'data' in ordine crescente\n",
    "air_quality.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Carica il file delle stazioni meteorologiche (adattalo al tuo percorso file)\n",
    "stazioni = pd.read_csv('./dataset/stazioni_mereologiche.csv')\n",
    "\n",
    "# Estraiamo gli inquinanti che ogni stazione può misurare e creiamo un dizionario\n",
    "stazioni['inquinanti'] = stazioni['inquinanti'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "dict_stazioni_inquinanti = stazioni.set_index('id_amat')['inquinanti'].to_dict()\n",
    "\n",
    "# Crea un nuovo DataFrame pivotato\n",
    "air_quality_pivoted = pd.pivot_table(air_quality, values='valore', index=['stazione_id', 'data'], columns='inquinante').reset_index()\n",
    "air_quality_pivoted['data'] = pd.to_datetime(air_quality_pivoted['data'])\n",
    "air_quality_pivoted.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Estrai l'anno da ogni data\n",
    "air_quality_pivoted['year'] = air_quality_pivoted['data'].dt.year\n",
    "\n",
    "# Calcolo delle mediane annuali e riempimento dei valori NaN\n",
    "annual_medians = {}\n",
    "for stazione_id, inquinanti in dict_stazioni_inquinanti.items():\n",
    "    for inquinante in inquinanti:\n",
    "        if inquinante in air_quality_pivoted.columns:\n",
    "            data_grouped = air_quality_pivoted[air_quality_pivoted['stazione_id'] == stazione_id].groupby('year')\n",
    "            medians_by_year = data_grouped[inquinante].median()\n",
    "\n",
    "            for year, median in medians_by_year.items():\n",
    "                annual_medians[(stazione_id, inquinante, year)] = median\n",
    "\n",
    "for key, median in annual_medians.items():\n",
    "    stazione_id, inquinante, year = key\n",
    "    mask = (air_quality_pivoted['stazione_id'] == stazione_id) & \\\n",
    "           (air_quality_pivoted['year'] == year) & \\\n",
    "           (air_quality_pivoted[inquinante].isna())\n",
    "    \n",
    "    air_quality_pivoted.loc[mask, inquinante] = median\n",
    "\n",
    "# Rimozione della colonna 'year' dopo aver completato le operazioni di imputazione\n",
    "air_quality_pivoted = air_quality_pivoted.drop(columns='year')\n",
    "air_quality_pivoted = air_quality_pivoted.fillna(\"N.D.\")\n",
    "\n",
    "# Salvataggio del DataFrame aggiornato\n",
    "air_quality_pivoted.to_csv('./dataset/rilevazione_qualità_aria/dataset_finale/air_quality_pivoted.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset rilevazione della qualità dell'aria`*\n",
    "\n",
    "Il codice qui presentato si occupa essenzialmente di trasformare un elenco di stazioni meteorologiche, con varie informazioni associate, in un formato facilmente utilizzabile per applicazioni geospaziali. Inizialmente, vengono eliminate alcune informazioni non necessarie, concentrando l'attenzione su dati più pertinenti, come la posizione geografica di ogni stazione.\n",
    "\n",
    "Una parte cruciale di questo processo riguarda la manipolazione delle coordinate geografiche. Dal momento che le posizioni sono state fornite come testo, il codice le converte in formato GeoJSON; uno standard specifico per i dati geografici.\n",
    "Questo formato è particolarmente utile perché è ampiamente riconosciuto e utilizzato in molte applicazioni di mappatura, rendendo i dati facilmente accessibili e utilizzabili.\n",
    "Alla fine di questa procedura avremo la possibilità, attraverso librerie e strumenti online, di ottenere una mappa con i nostri specifici \"Point\" e i relativi metadati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioni_metereologiche = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "stazioni_metereologiche.drop(columns=['inizio_operativita', 'fine_operativita','LONG_X_4326','LAT_Y_4326'], inplace=True)\n",
    "\n",
    "    # Estrai le coordinate dalla colonna \"Location\"\n",
    "stazioni_metereologiche['coordinates'] = stazioni_metereologiche['Location'].str.strip('()').str.split(', ')\n",
    "stazioni_metereologiche['latitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[0]))\n",
    "stazioni_metereologiche['longitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[1]))\n",
    "\n",
    "    # Crea la struttura base GeoJSON\n",
    "geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in stazioni_metereologiche.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['_id', 'Location', 'coordinates', 'latitude', 'longitude']).to_dict()\n",
    "    }\n",
    "    geojson[\"features\"].append(feature)\n",
    "\n",
    "    # Salva il GeoJSON in un file\n",
    "with open(\"./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.geojson\", 'w') as f:\n",
    "    json.dump(geojson, f)\n",
    "\n",
    "stazioni_metereologiche.to_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`3.2 - Arricchimento`**\n",
    "\n",
    "Successivamente alla fase di pulizia e selezione, si è introdotta una fase di arricchimento dei dataset.\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4. - Finalizzazione dei dataset`**\n",
    "\n",
    "#### **`4.1 - Ontologia`**\n",
    "\n",
    "???\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **`4.2 - Conversione in RDF`**\n",
    "\n",
    "???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **`5. - Visualizzazione dati`**\n",
    "\n",
    "???\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
