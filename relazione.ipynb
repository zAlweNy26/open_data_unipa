{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><strong>Relazione di Open Data Management 2023-2024</strong></h1>\n",
    "<h3 align=\"center\"><strong><em>di Daniele Nicosia e Claudio Bellanti</em></strong></h3>\n",
    "<h5 align=\"center\"><em>Università degli Studi di Palermo - Facoltà di Informatica</em></h5>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "### [`1. - Traccia`](#1-traccia)\n",
    "### [`2. - Selezione dei dataset`](#2-selezione-dei-dataset)\n",
    "- ##### [`2.1 - Raccolta`](#21-raccolta)\n",
    "- ##### [`2.2 - Licenze`](#22-licenze)\n",
    "### [`3. - Elaborazione dei dataset`](#3-elaborazione-dei-dataset)\n",
    "- ##### [`3.1 - Pulizia e selezione dei dati rilevanti`](#31-pulizia-e-selezione-dei-dati-rilevanti)\n",
    "- ##### [`3.2 - Arricchimento`](#32-arricchimento)\n",
    "### [`4. - Finalizzazione dei dataset`](#4-finalizzazione-dei-dataset)\n",
    "- ##### [`4.1 - Ontologia`](#41-ontologia)\n",
    "- ##### [`4.2 - Conversione in RDF`](#42-conversione-in-rdf)\n",
    "### [`5. - Visualizzazione dati`](#5-visualizzazione-dati)\n",
    "- ##### [`5.1 - Creazione mappe con i GeoJson`](#51-creazione-mappe-con-i-geojson)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1. - Traccia`**\n",
    "\n",
    "Utilizzando il linguaggio di programmazione Python, per lo sviluppo del progetto si devono innanzitutto rispettare i seguenti passi:\n",
    "\n",
    "- _Selezione dati_\n",
    "- _Elaborazione dati (data cleaning, definizione struttura omogenea)_\n",
    "- _Open Linked Data (creazione di uno strato semantico, ontologie, interlinking)_\n",
    "\n",
    "L'obiettivo della suddetta relazione è lo sviluppo di ???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2. - Selezione dei dataset`**\n",
    "\n",
    "#### **`2.1 - Raccolta`**\n",
    "\n",
    "Per lo sviluppo del progetto sono stati selezionati dei dataset i cui dati rappresentino gli eventi che si sono verificati nel Comune di Milano nel periodo 2014-2021. I dataset selezionati sono i seguenti:\n",
    "\n",
    "- [**`Parco veicoli circolanti per classe di inquinamento 2007-2022`**](https://dati.comune.milano.it/dataset/ds530-emissioni-inquinanti-standard-eu-2007-avanti)\n",
    "- [**`Ricoveri ordinari apparato respiratorio 2007-2021`**](https://dati.comune.milano.it/dataset/ds1053_ricoveri-ordinari-apparato-respiratorio)\n",
    "- [**`Stazioni di monitoraggio inquinanti`**](https://dati.comune.milano.it/dataset/ds484_stazioni_di_monitoraggio_inquinanti_atmosferici_dellarpa_sit)\n",
    "- [**`Ricostruzione della popolazione 2002-2019`**](https://demo.istat.it/app/?i=RIC&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2002 al 2019.\n",
    "- [**`Popolazione residente 2019-2023`**](https://demo.istat.it/app/?i=POS&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2020 al 2022.\n",
    "- [**`Standard di emissioni CO europei`**](https://en.wikipedia.org/wiki/European_emission_standards#Toxic_emission:_stages_and_legal_framework)\n",
    "  - Per questo dataset è stato necessario effettuare uno scraping del sito. Il dataset risultante contiene gli standard di emissioni CO europei dal 1992 al 2021.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`2.2 - Licenze`**\n",
    "\n",
    "Le licenze dei dataset selezionati sono le seguenti:\n",
    "\n",
    "- **`Parco veicoli circolanti per classe di inquinamento 2007-2022`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricoveri ordinari apparato respiratorio 2007-2021`**: Creative Commons Attribuzione-Condividi allo stesso modo 3.0 Italia ([CC BY-SA 3.0 IT](https://creativecommons.org/licenses/by-sa/3.0/it/))\n",
    "- **`Stazioni di monitoraggio inquinanti`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricostruzione della popolazione 2002-2019`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://www.istat.it/it/note-legali))\n",
    "- **`Popolazione residente 2019-2023`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Standard di emissioni CO europei`**: Creative Commons Attribuzione-ShareAlike 4.0 Internazionale ([CC BY-SA 4.0](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License))\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3. - Elaborazione dei dataset`**\n",
    "\n",
    "#### **`3.1 - Pulizia e selezione dei dati rilevanti`**\n",
    "\n",
    "Il primo passo effettuato è stato unire i diversi dataset della popolazione residente nel Comune di Milano, in un unico dataset.\n",
    "\n",
    "Si è notato che i dataset puri non erano tutti adattati allo stesso modo, infatti, si sono riscontrate due problematiche:\n",
    "\n",
    "- Encoding dei dataset differenti, a causa delle loro diversi origini\n",
    "- Errori vari dovuti a `;` o `,` non correttamente inseriti\n",
    "\n",
    "Inoltre, per il dataset delle emissioni è stato necessario effettuare uno scraping poichè non era disponibile in formato CSV. Per effettuare lo scraping è stato utilizzato il linguaggio di programmazione Python e la libreria `BeautifulSoup`. Lo script utilizzato è accessibile qui: [**`scraping.ipynb`**](dataset/emissioni/scraping.ipynb).\n",
    "\n",
    "##### *`Preparazione ambiente di sviluppo`*\n",
    "\n",
    "Prima di cominciare ad analizzare i dati dobbiamo importare le librerie necessarie al nostro scopo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset rilevazione della qualità dell'aria`*\n",
    "\n",
    "In questo estratto di codice, ci siamo concentrati sull'elaborazione di un insieme di dati complesso riguardante la qualità dell'aria, raccolto nel corso di diversi anni. Questi dati provengono da molteplici stazioni che, è importante notare, hanno capacità di rilevamento diverse. Non tutte le stazioni sono equipaggiate per misurare ogni tipo di inquinante, il che introduce una certa variabilità nei dati che stiamo trattando.\n",
    "\n",
    "Il primo passo significativo nel nostro processo è stato unire questi diversi file di dati, ognuno contenente frammenti del quadro generale della qualità dell'aria, in un unico, coerente DataFrame. Questo non solo ha semplificato le fasi successive dell'analisi, ma ha anche assicurato che ogni pezzo di informazione fosse contestualizzato all'interno del dataset più ampio.\n",
    "\n",
    "Una volta consolidati i dati, abbiamo affrontato la questione della uniformità temporale, assicurandoci che ogni data fosse in un formato standard e ordinando poi il dataset in ordine cronologico. Questo ordine temporale è essenziale per qualsiasi analisi storica o trend che potrebbe emergere da questi dati.\n",
    "\n",
    "Il problema successivo che abbiamo affrontato è stato quello dei valori mancanti, un'incidenza comune nei grandi set di dati, specialmente quando si tratta di rilevamenti ambientali su più anni. La strategia adottata qui è stata calcolare la mediana dei valori per ogni inquinante per ogni anno, una tecnica scelta per la sua resistenza agli outlier, ovvero quei valori che differiscono significativamente dalla norma e che potrebbero distorcere i risultati se non trattati correttamente.\n",
    "\n",
    "Utilizzando le mediane annuali, abbiamo potuto imputare valori ragionevoli nei casi in cui i dati erano mancanti, mantenendo l'integrità generale dei dati senza concedere un peso eccessivo a misurazioni anomale o inaccurate. Questo passaggio è essenziale per mantenere la validità delle nostre conclusioni finali.\n",
    "\n",
    "Dopo aver riempito questi vuoti, il nostro set di dati era quasi pronto per essere utilizzato in analisi future. Abbiamo rimosso alcune colonne superflue per rendere il dataset più snello e abbiamo gestito eventuali valori mancanti residui, assicurandoci che fossero chiaramente indicati.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('./data/raw/rilevazione_qualità_aria/*.csv')\n",
    "\n",
    "# Carica tutti i file CSV in un dizionario di DataFrame, specificando il separatore corretto\n",
    "dfs = {fp: pd.read_csv(fp, sep=';') for fp in file_paths}\n",
    "\n",
    "# Concatena tutti i DataFrame in un unico DataFrame\n",
    "air_quality = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Converti la colonna 'data' in datetime se non lo è già\n",
    "air_quality['data'] = pd.to_datetime(air_quality['data'])\n",
    "\n",
    "# Ordina il DataFrame in base alla colonna 'data' in ordine crescente\n",
    "air_quality.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Carica il file delle stazioni meteorologiche (adattalo al tuo percorso file)\n",
    "stazioni = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "# Estraiamo gli inquinanti che ogni stazione può misurare e creiamo un dizionario\n",
    "stazioni['inquinanti'] = stazioni['inquinanti'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "dict_stazioni_inquinanti = stazioni.set_index('id_amat')['inquinanti'].to_dict()\n",
    "\n",
    "# Crea un nuovo DataFrame pivotato\n",
    "air_quality_pivoted = pd.pivot_table(air_quality, values='valore', index=['stazione_id', 'data'], columns='inquinante').reset_index()\n",
    "air_quality_pivoted['data'] = pd.to_datetime(air_quality_pivoted['data'])\n",
    "air_quality_pivoted.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Estrai l'anno da ogni data\n",
    "air_quality_pivoted['year'] = air_quality_pivoted['data'].dt.year\n",
    "\n",
    "# Calcolo delle mediane annuali e riempimento dei valori NaN\n",
    "annual_medians = {}\n",
    "for stazione_id, inquinanti in dict_stazioni_inquinanti.items():\n",
    "    for inquinante in inquinanti:\n",
    "        if inquinante in air_quality_pivoted.columns:\n",
    "            data_grouped = air_quality_pivoted[air_quality_pivoted['stazione_id'] == stazione_id].groupby('year')\n",
    "            medians_by_year = data_grouped[inquinante].median()\n",
    "\n",
    "            for year, median in medians_by_year.items():\n",
    "                annual_medians[(stazione_id, inquinante, year)] = median\n",
    "\n",
    "for key, median in annual_medians.items():\n",
    "    stazione_id, inquinante, year = key\n",
    "    mask = (air_quality_pivoted['stazione_id'] == stazione_id) & \\\n",
    "           (air_quality_pivoted['year'] == year) & \\\n",
    "           (air_quality_pivoted[inquinante].isna())\n",
    "    \n",
    "    air_quality_pivoted.loc[mask, inquinante] = median\n",
    "\n",
    "# Rimozione della colonna 'year' dopo aver completato le operazioni di imputazione\n",
    "air_quality_pivoted = air_quality_pivoted.drop(columns='year')\n",
    "air_quality_pivoted = air_quality_pivoted.fillna(\"N.D.\")\n",
    "\n",
    "# Salvataggio del DataFrame aggiornato\n",
    "air_quality_pivoted.to_csv('./data/processed/rilevazione_qualità_aria/air_quality_pivoted.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson stazioni metereologiche`*\n",
    "\n",
    "Il codice qui presentato si occupa essenzialmente di trasformare un elenco di stazioni meteorologiche, con varie informazioni associate, in un formato facilmente utilizzabile per applicazioni geospaziali. Inizialmente, vengono eliminate alcune informazioni non necessarie, concentrando l'attenzione su dati più pertinenti, come la posizione geografica di ogni stazione.\n",
    "\n",
    "Una parte cruciale di questo processo riguarda la manipolazione delle coordinate geografiche. Dal momento che le posizioni sono state fornite come testo, il codice le converte in formato GeoJSON; uno standard specifico per i dati geografici.\n",
    "Questo formato è particolarmente utile perché è ampiamente riconosciuto e utilizzato in molte applicazioni di mappatura, rendendo i dati facilmente accessibili e utilizzabili.\n",
    "Alla fine di questa procedura avremo la possibilità, attraverso librerie e strumenti online, di ottenere una mappa con i nostri specifici \"Point\" e i relativi metadati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioni_metereologiche = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "stazioni_metereologiche.drop(columns=['inizio_operativita', 'fine_operativita','LONG_X_4326','LAT_Y_4326'], inplace=True)\n",
    "\n",
    "# Estrai le coordinate dalla colonna \"Location\"\n",
    "stazioni_metereologiche['coordinates'] = stazioni_metereologiche['Location'].str.strip('()').str.split(', ')\n",
    "stazioni_metereologiche['latitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[0]))\n",
    "stazioni_metereologiche['longitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[1]))\n",
    "\n",
    "# Crea la struttura base GeoJSON\n",
    "geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in stazioni_metereologiche.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['_id', 'Location', 'coordinates', 'latitude', 'longitude']).to_dict()\n",
    "    }\n",
    "    geojson[\"features\"].append(feature)\n",
    "\n",
    "# Salva il GeoJSON in un file\n",
    "with open(\"./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.geojson\", 'w') as f:\n",
    "    json.dump(geojson, f)\n",
    "\n",
    "stazioni_metereologiche.to_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson aree verdi`*\n",
    "\n",
    "Al fine di effettuare una analisi completa, abbiamo optato di inserire all'interno dell'analisi anche le aree verdi di Milano.\n",
    "In questo caso abbiamo prelevato i dati forniti in CSV dal comune di Milano e ne abbiamo realizzato un Geo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aree_verdi = pd.read_csv('./data/raw/rilevazione_qualità_aria/aree_verdi.csv')\n",
    "if 'Location' in aree_verdi.columns:\n",
    "    aree_verdi['coordinates'] = aree_verdi['Location'].str.strip('()').str.split(', ')\n",
    "    aree_verdi['latitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[0]))\n",
    "    aree_verdi['longitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[1]))\n",
    "else:\n",
    "    # Se non esiste una colonna 'Location', le coordinate potrebbero essere presenti come colonne separate\n",
    "    # Assicurati che le colonne 'latitude' e 'longitude' esistano nel tuo file CSV\n",
    "    aree_verdi['latitude'] = aree_verdi['latitude'].apply(float)  # Converti in float se già non lo sono\n",
    "    aree_verdi['longitude'] = aree_verdi['longitude'].apply(float)  # Converti in float se già non lo sono\n",
    "\n",
    "# Crea la struttura base GeoJSON\n",
    "geojson_aree_verdi = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in aree_verdi.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['Location', 'coordinates', 'latitude', 'longitude']).to_dict()  # escludi le colonne non necessarie\n",
    "    }\n",
    "    geojson_aree_verdi[\"features\"].append(feature)\n",
    "\n",
    "# Salva il GeoJSON in un file\n",
    "output_file = \"./data/processed/rilevazione_qualità_aria/aree_verdi.geojson\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(geojson_aree_verdi, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset emissioni veicoli`*\n",
    "Il dataset in questione sulle emissioni dei veicoli basata sulla classificazione EURO è stato ottenuto effettuando web scraping.\n",
    "Con esso siamo riusciti a prelevare da Wikipedia diverse tabelle dedicate a diversi tipi di veicolo, nonostante ciò il loro contenuto risultava particolarmente sporco e non completo.\n",
    "Quindi, prima di poter utilizzare questi dati abbiamo dovuto effettuare un lavoro di pulizia manuale, in quanto automatizzare l'operazione risultava particolarmente onerosa e non la scelta ideale.\n",
    "Per alcuni dataset è stato cambiato il delimitatore in quanto era utilizzato il punto e virgola, per una maggiore coerenza è stata sostituita con la virgola.\n",
    "Sono stati inoltre standardizzati i valori della colonna \"Tier\" in quanto erano presenti in alcuni CSV i numeri romani piuttosto che quelli arabi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/emissioni/'\n",
    "percorso_output = './data/processed/emissioni/'\n",
    "\n",
    "# Assicurati che le cartelle esistano, se no, creale\n",
    "os.makedirs(percorso_output, exist_ok=True)\n",
    "\n",
    "# Trova tutti i file CSV nel percorso di input\n",
    "files = glob.glob(os.path.join(percorso_input, '*.csv'))\n",
    "\n",
    "for file in files:\n",
    "    file_name = os.path.basename(file)\n",
    "    if file_name == 'standards_for_motor_cycles.csv':\n",
    "        df = pd.read_csv(file, delimiter=',')\n",
    "        if 'Standard' in df.columns:\n",
    "            df.rename(columns={'Standard': 'Tier'}, inplace=True)\n",
    "        df.to_csv(os.path.join(percorso_output, file_name), index=False, na_rep='NaN')\n",
    "        continue  # Salta questo file\n",
    "\n",
    "    # Leggi il CSV corrente\n",
    "    df = pd.read_csv(file, delimiter=';')\n",
    "\n",
    "    # Preparare il percorso del file di output\n",
    "    nome_file = os.path.basename(file)  # Estrae il nome del file dal percorso\n",
    "    file_di_output = os.path.join(percorso_output, nome_file)\n",
    "\n",
    "    #Droppa le colonne che non ci servono\n",
    "    if 'Date (type approval)' in df.columns:\n",
    "        df.drop(columns=['Date (type approval)'], axis=0, inplace=True)\n",
    "    #Cambiamo il nome delle colonne strane xd\n",
    "    if 'Date (first registration)' in df.columns:\n",
    "        df.rename(columns={'Date (first registration)': 'Date'}, inplace=True)\n",
    "    if 'Standard' in df.columns:\n",
    "        df.rename(columns={'Standard': 'Tier'}, inplace=True)\n",
    "\n",
    "    # Elimino tutto ciò che non è un numero in tutte le righe della colonna Date\n",
    "    df['Date'] = df['Date'].apply(lambda x: re.sub('[^0-9]', '', str(x)))\n",
    "\n",
    "    df.to_csv(file_di_output, index=False, sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset veicoli per classe di inquinamento`*\n",
    "\n",
    "Il seguente dataset è stato ottenuto rimuovendo le colonne degli anni che non ci interessava analizzare. Inoltre le colonne sono state convertite in title-case e sono stati sostituiti eventuali valori NaN con 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/veicoli/categorie_euro_2007_2022.csv'\n",
    "percorso_output = './data/processed/veicoli/categorie_euro_2014_2021.csv'\n",
    "\n",
    "veicoli = pd.read_csv(percorso_input, sep=';')\n",
    "\n",
    "veicoli = veicoli[veicoli['ANNO'].between(2014, 2021)]\n",
    "\n",
    "veicoli = veicoli.reset_index(drop=True)\n",
    "\n",
    "veicoli = veicoli.fillna(0)\n",
    "\n",
    "veicoli.columns = veicoli.columns.str.title()\n",
    "\n",
    "veicoli.to_csv(percorso_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset popolazione`*\n",
    "\n",
    "Il dataset in questione è stato ottenuto, innanzitutto, rimuovendo le colonne degli anni che non ci interessava analizzare. Successivamente, è stato necessario unire dei dataset poichè sono stati cambiati i metadati e la loro disposizione negli anni successivi al 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/popolazione/'\n",
    "percorso_output = './data/processed/popolazione/'\n",
    "\n",
    "popolazione = pd.read_csv(percorso_input + 'ricostruzione_popolazione_2002_2019.csv')\n",
    "\n",
    "anni = np.arange(2014, 2021)\n",
    "for col in popolazione.columns:\n",
    "    try:\n",
    "        year = int(col)\n",
    "        if year not in anni:\n",
    "            popolazione = popolazione.drop(columns=col)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "file_list = os.listdir(percorso_input)\n",
    "popolazione_files = [f for f in file_list if f.startswith('popolazione')]\n",
    "\n",
    "prev_age = None\n",
    "curr_row = None\n",
    "\n",
    "for filename in popolazione_files:\n",
    "    new_pop = pd.read_csv(os.path.join(percorso_input, filename))\n",
    "\n",
    "    year = filename[-8:-4]\n",
    "    popolazione[year] = 0\n",
    "\n",
    "    for index, row in popolazione.iterrows():\n",
    "        age = row['Età']\n",
    "        if age != prev_age:\n",
    "            curr_row = new_pop.loc[new_pop['Età'] == age].iloc[0]\n",
    "            prev_age = age\n",
    "\n",
    "        if popolazione.loc[index, 'Sesso'] == 'Maschi':\n",
    "            popolazione.loc[index, year] = curr_row['Totale maschi']\n",
    "        elif popolazione.loc[index, 'Sesso'] == 'Femmine':\n",
    "            popolazione.loc[index, year] = curr_row['Totale femmine']\n",
    "        else:\n",
    "            popolazione.loc[index, year] = curr_row['Totale']\n",
    "\n",
    "popolazione.to_csv(percorso_output + 'popolazione_2014_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`3.2 - Arricchimento`**\n",
    "\n",
    "Successivamente alla fase di pulizia e selezione, si è introdotta una fase di arricchimento dei dataset.\n",
    "\n",
    "##### *`Arricchimento dataset emissioni`*\n",
    "\n",
    "Poichè non siamo riusciti a trovare un dataset che contenesse il parco di veicoli in base ai kWh o alla cilindrata, abbiamo deciso di arricchire il dataset delle emissioni con le informazioni circa la media di questi valori, per ogni specifica classe di inquinamento. Abbiamo impostato quindi i valori in questo modo:\n",
    "\n",
    "| Classe Euro   | kWh   | Media km percorsi |\n",
    "|:-------------:|:-----:|:-----------------:|\n",
    "| Euro 0        | 55    | 20                |\n",
    "| Euro 1        | 60    | 20                |\n",
    "| Euro 2        | 65    | 20                |\n",
    "| Euro 3        | 70    | 20                |\n",
    "| Euro 4        | 75    | 40                |\n",
    "| Euro 5        | 80    | 50                |\n",
    "| Euro 6        | 85    | 60                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4. - Finalizzazione dei dataset`**\n",
    "\n",
    "#### **`4.1 - Ontologia`**\n",
    "\n",
    "L'ontologia progettata in questione è di tipo `OWL` ed è stata realizzata, con l'ausilio del software `Protégé`, in modo da poter essere utilizzata per la rappresentazione di dati relativi alla qualità dell'aria in un determinato comune. Nell'ontologia sono rappresentate le seguenti principali entità o concetti:\n",
    "\n",
    "- **`Comune`**: che rappresenta i comuni.\n",
    "- **`Popolazione`**: che rappresenta la popolazione.\n",
    "- **`AreaVerde`**: che rappresenta le aree verdi.\n",
    "- **`StazioneMeteorologica`**: che rappresenta le stazioni meteorologiche.\n",
    "- **`MisurazioneQualitaAria`**: che rappresenta le misurazioni della qualità dell'aria.\n",
    "- **`Veicolo`**: che rappresenta i veicoli.\n",
    "- **`ClasseInquinamento`**: che rappresenta le classi di inquinamento.\n",
    "\n",
    "Sono state definite anche delle sottoclassi per categorizzare ulteriormente i veicoli (Motocicli, Autovetture, Autobus, Motrici, Industriali) e le classi di inquinamento (Euro0, Euro1, Euro2, Euro3, Euro4, Euro5, Euro6).\n",
    "\n",
    "- **`haClasseInquinamento`**: Collega le entità per rappresentare l'appartenenza di un veicolo ad una classe di inquinamento specifica.\n",
    "- **`haLimiteInquinante`**: Specifica il limite numerico associato a una classe di inquinamento.\n",
    "- **`haRicoveri`**: Rappresenta il numero di ricoveri associati ad una popolazione.\n",
    "- **`haPopolazione`**: Stabilisce il numero di abitanti per un dato comune.\n",
    "- **`haAreaVerde`**: Indica la presenza di aree verdi all'interno di un comune.\n",
    "- **`haStazioneMeteorologica`**: Rappresenta la presenza di stazioni meteorologiche in un comune.\n",
    "- **`localizzataIn`**: Indica la posizione geografica di una stazione meteorologica o di un'area verde.\n",
    "- **`monitoraInquinante`**: Specifica l'inquinante specifico misurato da una stazione meteorologica.\n",
    "- **`haValoreInquinante`**: Indica il valore numerico effettivo di una misurazione della qualità dell'aria registrato da una stazione meteorologica.\n",
    "- **`haQualitaAria`**: Collega un'area verde alle misurazioni della qualità dell'aria effettuate in quella zona.\n",
    "- **`haDensitaPopolazione`**: Rappresenta la densità di popolazione in un'area verde.\n",
    "- **`latitudine`** e **`longitudine`**: Specifica le coordinate geografiche (latitudine e longitudine) di stazioni meteorologiche e aree verdi.\n",
    "\n",
    "\n",
    "L'ontologia descritta è stata realizzata nel formato `.ttl` ed è riportata in [**`questo file`**](ontologia.ttl).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse \n",
    "from rdflib import Graph, Literal, URIRef, Namespace\n",
    "from rdflib.namespace import RDF, OWL, RDFS\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "base_uri = \"http://www.sanitasicilia.it/resource/\"\n",
    "\n",
    "sso = Namespace(\"http://www.semanticweb.org/aria-ontology-cb-dn/ontology/\")\n",
    "g.bind(\"sso\", sso)\n",
    "\n",
    "ssr = Namespace(\"http://www.semanticweb.org/aria-ontology-cb-dn/resource/\")\n",
    "g.bind(\"ssr\", ssr)\n",
    "\n",
    "def urify(uri, res):\n",
    "    res = res.replace(\" \",\"_\").replace(\"\\'\",\"\")\n",
    "    return uri + parse.quote(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carica il dataset sulla qualità dell'aria\n",
    "air_quality_df = pd.read_csv('./data/processed/rilevazione_qualità_aria/air_quality_pivoted.csv')\n",
    "\n",
    "# Prepara il grafo RDF\n",
    "g = Graph()\n",
    "\n",
    "# Prepara i namespace\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "# Prepara la query SPARQL\n",
    "def get_wikidata_uris(pollutant_names):\n",
    "    uris = []\n",
    "    for pollutant_name in pollutant_names:\n",
    "        encoded_pollutant_name = parse.quote_plus(pollutant_name)\n",
    "        sparql_query = \"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?item WHERE {{\n",
    "        ?item rdfs:label \"{0}\"@en.\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(pollutant_name)  # Non codificare qui, SPARQLWrapper gestisce già l'encoding\n",
    "        sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        sparql.setQuery(sparql_query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        try:\n",
    "            results = sparql.query().convert()\n",
    "            for result in results[\"results\"][\"bindings\"]:\n",
    "                uris.append(result[\"item\"][\"value\"])\n",
    "                break  # Break here ensures that only the first result is taken\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while querying {pollutant_name}: {e}\")\n",
    "            uris.append(None)  # Append None if there was an error or no result\n",
    "\n",
    "    return uris\n",
    "\n",
    "# Utilizza la nuova funzione per ottenere tutti gli URI\n",
    "arr_inquinanti = [\"benzene\",\"carbon monoxide\",\"nitrogen dioxide\",\"ozone\",\"PM10 (including PM 2.5)\",\"PM10 (including PM 2.5)\",\"sulfur dioxide\"]\n",
    "pollutant_uris = get_wikidata_uris(arr_inquinanti)\n",
    "\n",
    "# Ora puoi iterare su pollutant_uris e creare le tue triple RDF\n",
    "g = Graph()\n",
    "for pollutant_name, pollutant_uri in zip(arr_inquinanti, pollutant_uris):\n",
    "    print(pollutant_uri)\n",
    "    if pollutant_uri:\n",
    "        # Crea l'URIRef con l'URI trovato\n",
    "        pollutant_uri_ref = URIRef(pollutant_uri)\n",
    "        # Aggiungi le triple al grafo\n",
    "        g.add((pollutant_uri_ref, RDF.type, OWL.Thing))\n",
    "        g.add((pollutant_uri_ref, RDFS.label, Literal(pollutant_name)))\n",
    "\n",
    "# Salva il grafo RDF\n",
    "g.serialize(destination='./data/ontology/linked_air_quality.ttl', format='turtle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **`4.2 - Conversione in RDF`**\n",
    "\n",
    "???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **`5. - Visualizzazione dati`**\n",
    "\n",
    "#### **`5.1 - Creazione mappe con i GeoJson`**\n",
    "\n",
    "Test\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
