{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><strong>Relazione di Open Data Management 2023-2024</strong></h1>\n",
    "<h3 align=\"center\"><strong><em>di Daniele Nicosia e Claudio Bellanti</em></strong></h3>\n",
    "<h5 align=\"center\"><em>Università degli Studi di Palermo - Facoltà di Informatica</em></h5>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "### [`1. - Traccia`](#1-traccia)\n",
    "### [`2. - Selezione dei dataset`](#2-selezione-dei-dataset)\n",
    "- ##### [`2.1 - Raccolta`](#21-raccolta)\n",
    "- ##### [`2.2 - Licenze`](#22-licenze)\n",
    "### [`3. - Elaborazione dei dataset`](#3-elaborazione-dei-dataset)\n",
    "- ##### [`3.1 - Pulizia e selezione dei dati rilevanti`](#31-pulizia-e-selezione-dei-dati-rilevanti)\n",
    "- ##### [`3.2 - Arricchimento`](#32-arricchimento)\n",
    "### [`4. - Finalizzazione dei dataset`](#4-finalizzazione-dei-dataset)\n",
    "- ##### [`4.1 - Ontologia`](#41-ontologia)\n",
    "- ##### [`4.2 - Conversione in RDF`](#42-conversione-in-rdf)\n",
    "### [`5. - Visualizzazione dati`](#5-visualizzazione-dati)\n",
    "- ##### [`5.1 - Creazione mappe con i GeoJson`](#51-creazione-mappe-con-i-geojson)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1. - Traccia`**\n",
    "\n",
    "Utilizzando il linguaggio di programmazione Python, per lo sviluppo del progetto si devono innanzitutto rispettare i seguenti passi:\n",
    "\n",
    "- _Selezione dati_\n",
    "- _Elaborazione dati (data cleaning, definizione struttura omogenea)_\n",
    "- _Open Linked Data (creazione di uno strato semantico, ontologie, interlinking)_\n",
    "\n",
    "L'obiettivo della suddetta relazione è lo studio della qualità dell'aria nel **`Comune di Milano`** in relazione alle aree verdi presenti sul territorio, con particolare attenzione alla presenza di polveri sottili (PM10) e di biossido di azoto (NO2).\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2. - Selezione dei dataset`**\n",
    "\n",
    "#### **`2.1 - Raccolta`**\n",
    "\n",
    "Per lo sviluppo del progetto sono stati selezionati dei dataset i cui dati rappresentino gli eventi che si sono verificati nel Comune di Milano nel periodo 2014-2021. I dataset selezionati sono i seguenti:\n",
    "\n",
    "- [**`Ricoveri ordinari apparato respiratorio 2007-2021`**](https://dati.comune.milano.it/dataset/ds1053_ricoveri-ordinari-apparato-respiratorio)\n",
    "- [**`Stazioni di monitoraggio inquinanti`**](https://dati.comune.milano.it/dataset/ds484_stazioni_di_monitoraggio_inquinanti_atmosferici_dellarpa_sit)\n",
    "- [**`Aree verdi`**]() TODO\n",
    "- [**`Ricostruzione della popolazione 2002-2019`**](https://demo.istat.it/app/?i=RIC&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2002 al 2019.\n",
    "- [**`Popolazione residente 2019-2023`**](https://demo.istat.it/app/?i=POS&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2020 al 2022.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`2.2 - Licenze`**\n",
    "\n",
    "Le licenze dei dataset selezionati sono le seguenti:\n",
    "\n",
    "- **`Ricoveri ordinari apparato respiratorio 2007-2021`**: Creative Commons Attribuzione-Condividi allo stesso modo 3.0 Italia ([CC BY-SA 3.0 IT](https://creativecommons.org/licenses/by-sa/3.0/it/))\n",
    "- **`Stazioni di monitoraggio inquinanti`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Aree verdi`**: TODO\n",
    "- **`Ricostruzione della popolazione 2002-2019`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://www.istat.it/it/note-legali))\n",
    "- **`Popolazione residente 2019-2023`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3. - Elaborazione dei dataset`**\n",
    "\n",
    "#### **`3.1 - Pulizia e selezione dei dati rilevanti`**\n",
    "\n",
    "Il primo passo effettuato è stato unire i diversi dataset della popolazione residente nel Comune di Milano, in un unico dataset.\n",
    "\n",
    "Si è notato che i dataset puri non erano tutti adattati allo stesso modo, infatti, si sono riscontrate due problematiche:\n",
    "\n",
    "- Encoding dei dataset differenti, a causa delle loro diversi origini\n",
    "- Errori vari dovuti a `;` o `,` non correttamente inseriti\n",
    "\n",
    "##### *`Preparazione ambiente di sviluppo`*\n",
    "\n",
    "Prima di cominciare ad analizzare i dati dobbiamo importare le librerie necessarie al nostro scopo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset rilevazione della qualità dell'aria`*\n",
    "\n",
    "In questo estratto di codice, ci siamo concentrati sull'elaborazione di un insieme di dati complesso riguardante la qualità dell'aria, raccolto nel corso di diversi anni. Questi dati provengono da molteplici stazioni che, è importante notare, hanno capacità di rilevamento diverse. Non tutte le stazioni sono equipaggiate per misurare ogni tipo di inquinante, il che introduce una certa variabilità nei dati che stiamo trattando.\n",
    "\n",
    "Il primo passo significativo nel nostro processo è stato unire questi diversi file di dati, ognuno contenente frammenti del quadro generale della qualità dell'aria, in un unico, coerente DataFrame. Questo non solo ha semplificato le fasi successive dell'analisi, ma ha anche assicurato che ogni pezzo di informazione fosse contestualizzato all'interno del dataset più ampio.\n",
    "\n",
    "Una volta consolidati i dati, abbiamo affrontato la questione della uniformità temporale, assicurandoci che ogni data fosse in un formato standard e ordinando poi il dataset in ordine cronologico. Questo ordine temporale è essenziale per qualsiasi analisi storica o trend che potrebbe emergere da questi dati.\n",
    "\n",
    "Il problema successivo che abbiamo affrontato è stato quello dei valori mancanti, un'incidenza comune nei grandi set di dati, specialmente quando si tratta di rilevamenti ambientali su più anni. La strategia adottata qui è stata calcolare la mediana dei valori per ogni inquinante per ogni anno, una tecnica scelta per la sua resistenza agli outlier, ovvero quei valori che differiscono significativamente dalla norma e che potrebbero distorcere i risultati se non trattati correttamente.\n",
    "\n",
    "Utilizzando le mediane annuali, abbiamo potuto imputare valori ragionevoli nei casi in cui i dati erano mancanti, mantenendo l'integrità generale dei dati senza concedere un peso eccessivo a misurazioni anomale o inaccurate. Questo passaggio è essenziale per mantenere la validità delle nostre conclusioni finali.\n",
    "\n",
    "Dopo aver riempito questi vuoti, il nostro set di dati era quasi pronto per essere utilizzato in analisi future. Abbiamo rimosso alcune colonne superflue per rendere il dataset più snello e abbiamo gestito eventuali valori mancanti residui, assicurandoci che fossero chiaramente indicati.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('./data/raw/rilevazione_qualità_aria/*.csv')\n",
    "\n",
    "# Carica tutti i file CSV in un dizionario di DataFrame, specificando il separatore corretto\n",
    "dfs = {fp: pd.read_csv(fp, sep=';') for fp in file_paths}\n",
    "\n",
    "# Concatena tutti i DataFrame in un unico DataFrame\n",
    "air_quality = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Converti la colonna 'data' in datetime se non lo è già\n",
    "air_quality['data'] = pd.to_datetime(air_quality['data'])\n",
    "\n",
    "# Ordina il DataFrame in base alla colonna 'data' in ordine crescente\n",
    "air_quality.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Carica il file delle stazioni meteorologiche (adattalo al tuo percorso file)\n",
    "stazioni = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "# Estraiamo gli inquinanti che ogni stazione può misurare e creiamo un dizionario\n",
    "stazioni['inquinanti'] = stazioni['inquinanti'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "dict_stazioni_inquinanti = stazioni.set_index('id_amat')['inquinanti'].to_dict()\n",
    "\n",
    "# Crea un nuovo DataFrame pivotato\n",
    "air_quality_pivoted = pd.pivot_table(air_quality, values='valore', index=['stazione_id', 'data'], columns='inquinante').reset_index()\n",
    "air_quality_pivoted['data'] = pd.to_datetime(air_quality_pivoted['data'])\n",
    "air_quality_pivoted.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Estrai l'anno da ogni data\n",
    "air_quality_pivoted['year'] = air_quality_pivoted['data'].dt.year\n",
    "\n",
    "# Calcolo delle mediane annuali e riempimento dei valori NaN\n",
    "annual_medians = {}\n",
    "for stazione_id, inquinanti in dict_stazioni_inquinanti.items():\n",
    "    for inquinante in inquinanti:\n",
    "        if inquinante in air_quality_pivoted.columns:\n",
    "            data_grouped = air_quality_pivoted[air_quality_pivoted['stazione_id'] == stazione_id].groupby('year')\n",
    "            medians_by_year = data_grouped[inquinante].median()\n",
    "\n",
    "            for year, median in medians_by_year.items():\n",
    "                annual_medians[(stazione_id, inquinante, year)] = median\n",
    "\n",
    "for key, median in annual_medians.items():\n",
    "    stazione_id, inquinante, year = key\n",
    "    mask = (air_quality_pivoted['stazione_id'] == stazione_id) & \\\n",
    "           (air_quality_pivoted['year'] == year) & \\\n",
    "           (air_quality_pivoted[inquinante].isna())\n",
    "    \n",
    "    air_quality_pivoted.loc[mask, inquinante] = median\n",
    "\n",
    "# Rimozione della colonna 'year' dopo aver completato le operazioni di imputazione\n",
    "air_quality_pivoted = air_quality_pivoted.drop(columns='year')\n",
    "air_quality_pivoted = air_quality_pivoted.fillna(\"N.D.\")\n",
    "\n",
    "# Salvataggio del DataFrame aggiornato\n",
    "air_quality_pivoted.to_csv('./data/processed/rilevazione_qualità_aria/air_quality_pivoted.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson stazioni metereologiche`*\n",
    "\n",
    "Il codice qui presentato si occupa essenzialmente di trasformare un elenco di stazioni meteorologiche, con varie informazioni associate, in un formato facilmente utilizzabile per applicazioni geospaziali. Inizialmente, vengono eliminate alcune informazioni non necessarie, concentrando l'attenzione su dati più pertinenti, come la posizione geografica di ogni stazione.\n",
    "\n",
    "Una parte cruciale di questo processo riguarda la manipolazione delle coordinate geografiche. Dal momento che le posizioni sono state fornite come testo, il codice le converte in formato GeoJSON; uno standard specifico per i dati geografici.\n",
    "Questo formato è particolarmente utile perché è ampiamente riconosciuto e utilizzato in molte applicazioni di mappatura, rendendo i dati facilmente accessibili e utilizzabili.\n",
    "Alla fine di questa procedura avremo la possibilità, attraverso librerie e strumenti online, di ottenere una mappa con i nostri specifici \"Point\" e i relativi metadati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioni_metereologiche = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "stazioni_metereologiche.drop(columns=['inizio_operativita', 'fine_operativita','LONG_X_4326','LAT_Y_4326'], inplace=True)\n",
    "\n",
    "# Estrai le coordinate dalla colonna \"Location\"\n",
    "stazioni_metereologiche['coordinates'] = stazioni_metereologiche['Location'].str.strip('()').str.split(', ')\n",
    "stazioni_metereologiche['latitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[0]))\n",
    "stazioni_metereologiche['longitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[1]))\n",
    "\n",
    "# Crea la struttura base GeoJSON\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in stazioni_metereologiche.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['_id', 'Location', 'coordinates', 'latitude', 'longitude']).to_dict()\n",
    "    }\n",
    "    geojson[\"features\"].append(feature)\n",
    "\n",
    "# Salva il GeoJSON in un file\n",
    "with open(\"./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.geojson\", 'w') as f:\n",
    "    json.dump(geojson, f)\n",
    "\n",
    "stazioni_metereologiche.to_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson aree verdi`*\n",
    "\n",
    "Al fine di effettuare una analisi completa, abbiamo optato di inserire all'interno dell'analisi anche le aree verdi di Milano.\n",
    "In questo caso abbiamo prelevato i dati forniti in CSV dal comune di Milano e ne abbiamo realizzato un Geo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aree_verdi = pd.read_csv('./data/raw/rilevazione_qualità_aria/aree_verdi.csv')\n",
    "if 'Location' in aree_verdi.columns:\n",
    "    aree_verdi['coordinates'] = aree_verdi['Location'].str.strip('()').str.split(', ')\n",
    "    aree_verdi['latitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[0]))\n",
    "    aree_verdi['longitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[1]))\n",
    "else:\n",
    "    # Se non esiste una colonna 'Location', le coordinate potrebbero essere presenti come colonne separate\n",
    "    # Assicurati che le colonne 'latitude' e 'longitude' esistano nel tuo file CSV\n",
    "    aree_verdi['latitude'] = aree_verdi['latitude'].apply(float)  # Converti in float se già non lo sono\n",
    "    aree_verdi['longitude'] = aree_verdi['longitude'].apply(float)  # Converti in float se già non lo sono\n",
    "\n",
    "# Crea la struttura base GeoJSON\n",
    "geojson_aree_verdi = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in aree_verdi.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['Location', 'coordinates', 'latitude', 'longitude']).to_dict()  # escludi le colonne non necessarie\n",
    "    }\n",
    "    geojson_aree_verdi[\"features\"].append(feature)\n",
    "\n",
    "# Salva il GeoJSON in un file\n",
    "output_file = \"./data/processed/rilevazione_qualità_aria/aree_verdi.geojson\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(geojson_aree_verdi, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset popolazione`*\n",
    "\n",
    "Il dataset in questione è stato ottenuto, innanzitutto, rimuovendo le colonne degli anni che non ci interessava analizzare. Successivamente, è stato necessario unire dei dataset poichè sono stati cambiati i metadati e la loro disposizione negli anni successivi al 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/popolazione/'\n",
    "percorso_output = './data/processed/popolazione/'\n",
    "\n",
    "popolazione = pd.read_csv(percorso_input + 'ricostruzione_popolazione_2002_2019.csv')\n",
    "\n",
    "anni = np.arange(2014, 2021)\n",
    "for col in popolazione.columns:\n",
    "    try:\n",
    "        year = int(col)\n",
    "        if year not in anni:\n",
    "            popolazione = popolazione.drop(columns=col)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "file_list = os.listdir(percorso_input)\n",
    "popolazione_files = [f for f in file_list if f.startswith('popolazione')]\n",
    "\n",
    "prev_age = None\n",
    "curr_row = None\n",
    "\n",
    "for filename in popolazione_files:\n",
    "    new_pop = pd.read_csv(os.path.join(percorso_input, filename))\n",
    "\n",
    "    year = filename[-8:-4]\n",
    "    popolazione[year] = 0\n",
    "\n",
    "    for index, row in popolazione.iterrows():\n",
    "        age = row['Età']\n",
    "        if age != prev_age:\n",
    "            curr_row = new_pop.loc[new_pop['Età'] == age].iloc[0]\n",
    "            prev_age = age\n",
    "\n",
    "        if popolazione.loc[index, 'Sesso'] == 'Maschi':\n",
    "            popolazione.loc[index, year] = curr_row['Totale maschi']\n",
    "        elif popolazione.loc[index, 'Sesso'] == 'Femmine':\n",
    "            popolazione.loc[index, year] = curr_row['Totale femmine']\n",
    "        else:\n",
    "            popolazione.loc[index, year] = curr_row['Totale']\n",
    "\n",
    "popolazione.to_csv(percorso_output + 'popolazione_2014_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset ricoveri`*\n",
    "\n",
    "Per il dataset dei ricoveri, poichè i dati erano già in un formato accettabile, è stato necessario solo rimuovere le colonne e le righe non necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/ricoveri/'\n",
    "percorso_output = './data/processed/ricoveri/'\n",
    "\n",
    "ricoveri = pd.read_csv(percorso_input + 'ricoveri_ordinari_apparato_respiratorio.csv', sep=';')\n",
    "\n",
    "# Rimuovo le righe degli anni precedenti al 2014\n",
    "ricoveri = ricoveri[ricoveri['anno'] >= 2014]\n",
    "\n",
    "# Rimuovo la colonna della misura utilizzata\n",
    "ricoveri = ricoveri.drop(columns='misura')\n",
    "\n",
    "ricoveri.to_csv(percorso_output + 'ricoveri_2014_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`3.2 - Arricchimento`**\n",
    "\n",
    "Successivamente alla fase di pulizia e selezione, si è introdotta una fase di arricchimento dei dataset.\n",
    "\n",
    "##### *`Arricchimento dataset [NOME]`*\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4. - Finalizzazione dei dataset`**\n",
    "\n",
    "#### **`4.1 - Ontologia`**\n",
    "\n",
    "L'ontologia progettata in questione è di tipo `OWL` ed è stata realizzata, con l'ausilio del software `Protégé`, in modo da poter essere utilizzata per la rappresentazione di dati relativi alla qualità dell'aria in un determinato comune. \n",
    "\n",
    "Nell'ontologia sono state rappresentate le seguenti entità:\n",
    "\n",
    "- **`Comune`**: che rappresenta i comuni.\n",
    "- **`Popolazione`**: che rappresenta la popolazione.\n",
    "- **`AreaVerde`**: che rappresenta le aree verdi.\n",
    "- **`StazioneMeteorologica`**: che rappresenta le stazioni meteorologiche.\n",
    "- **`MisurazioneQualitaAria`**: che rappresenta le misurazioni della qualità dell'aria.\n",
    "\n",
    "Inoltre le proprietà rappresentate nell'ontologia sono:\n",
    "\n",
    "- **`haRicoveri`**: Rappresenta il numero di ricoveri associati ad una popolazione.\n",
    "- **`haPopolazione`**: Stabilisce il numero di abitanti per un dato comune.\n",
    "- **`haAreaVerde`**: Indica la presenza di aree verdi all'interno di un comune.\n",
    "- **`haStazioneMeteorologica`**: Rappresenta la presenza di stazioni meteorologiche in un comune.\n",
    "- **`haCoordinate`**: Indica la posizione geografica di una stazione meteorologica o di un'area verde.\n",
    "- **`monitoraInquinante`**: Specifica l'inquinante specifico misurato da una stazione meteorologica.\n",
    "- **`haValoreInquinante`**: Indica il valore numerico effettivo di una misurazione della qualità dell'aria registrato da una stazione meteorologica.\n",
    "- **`haQualitaAria`**: Collega un'area verde alle misurazioni della qualità dell'aria effettuate in quella zona.\n",
    "- **`haDensitaPopolazione`**: Rappresenta la densità di popolazione in un'area verde.\n",
    "- **`latitudine`** e **`longitudine`**: Specifica le coordinate geografiche (latitudine e longitudine) di stazioni meteorologiche e aree verdi.\n",
    "\n",
    "\n",
    "L'ontologia descritta è stata realizzata nel formato `.ttl` ed è riportata in [**`questo file`**](ontologia.ttl).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse \n",
    "from rdflib import Graph, Literal, URIRef, Namespace\n",
    "from rdflib.namespace import RDF, OWL, RDFS\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from rdflib import XSD\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "base_uri = \"http://www.sanitasicilia.it/resource/\"\n",
    "\n",
    "sso = Namespace(\"http://www.semanticweb.org/aria-ontology-cb-dn/ontology/\")\n",
    "g.bind(\"sso\", sso)\n",
    "\n",
    "ssr = Namespace(\"http://www.semanticweb.org/aria-ontology-cb-dn/resource/\")\n",
    "g.bind(\"ssr\", ssr)\n",
    "\n",
    "def urify(uri, res):\n",
    "    res = res.replace(\" \",\"_\").replace(\"\\'\",\"\")\n",
    "    return uri + parse.quote(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Aggiungere commenti, in questo caso sto facendo l'interlinking degli inquinanti con wikidata\n",
    "# Carica il dataset sulla qualità dell'aria\n",
    "import time\n",
    "\n",
    "\n",
    "air_quality_df = pd.read_csv('./data/processed/rilevazione_qualità_aria/air_quality_pivoted.csv')\n",
    "\n",
    "# Prepara il grafo RDF\n",
    "#g = Graph()\n",
    "\n",
    "# Prepara i namespace\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "\n",
    "# Prepara la query SPARQL\n",
    "def get_wikidata_uris(pollutant_names):\n",
    "    uris = []\n",
    "    for pollutant_name in pollutant_names:\n",
    "        encoded_pollutant_name = parse.quote_plus(pollutant_name)\n",
    "        sparql_query = \"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?item WHERE {{\n",
    "        ?item rdfs:label \"{0}\"@en.\n",
    "        }}\n",
    "        LIMIT 1\n",
    "        \"\"\".format(pollutant_name)  # Non codificare qui, SPARQLWrapper gestisce già l'encoding\n",
    "        sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        sparql.setQuery(sparql_query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        try:\n",
    "            results = sparql.query().convert()\n",
    "            for result in results[\"results\"][\"bindings\"]:\n",
    "                uris.append(result[\"item\"][\"value\"])\n",
    "                time.sleep(1)\n",
    "                break  # Break here ensures that only the first result is taken\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while querying {pollutant_name}: {e}\")\n",
    "            uris.append(None)  # Append None if there was an error or no result\n",
    "\n",
    "    return uris\n",
    "\n",
    "# Utilizza la nuova funzione per ottenere tutti gli URI\n",
    "arr_inquinanti = [\"benzene\",\"carbon monoxide\",\"nitrogen dioxide\",\"ozone\",\"PM10 (including PM 2.5)\",\"PM10 (including PM 2.5)\",\"sulfur dioxide\"]\n",
    "pollutant_uris = get_wikidata_uris(arr_inquinanti)\n",
    "\n",
    "# Ora puoi iterare su pollutant_uris e creare le tue triple RDF\n",
    "#g = Graph()\n",
    "for pollutant_name, pollutant_uri in zip(arr_inquinanti, pollutant_uris):\n",
    "    print(pollutant_uri)\n",
    "    if pollutant_uri:\n",
    "        # Crea l'URIRef con l'URI trovato\n",
    "        pollutant_uri_ref = URIRef(pollutant_uri)\n",
    "        # Aggiungi le triple al grafo\n",
    "        g.add((pollutant_uri_ref, RDF.type, OWL.Thing))\n",
    "        g.add((pollutant_uri_ref, RDFS.label, Literal(pollutant_name)))\n",
    "# Salva il grafo RDF\n",
    "g.serialize(destination='./data/ontology/linked_air_quality.ttl', format='turtle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### DA ELIMINARE      ########################################################\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "\n",
    "def get_area_verde_uris(valori_sparql):\n",
    "    query = f\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?entita WHERE {{\n",
    "            VALUES ?nomeRicerca {{ {valori_sparql} }}\n",
    "            ?entita rdfs:label ?nomeRicerca .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "            # Esegui la query\n",
    "            results = sparql.query().convert()\n",
    "            # Estrai gli URI dalle risposte\n",
    "            uris = [result['entita']['value'] for result in results[\"results\"][\"bindings\"]]\n",
    "            return uris\n",
    "    except Exception as e:\n",
    "            print(f\"Si è verificato un errore durante l'esecuzione della query: {e}\")\n",
    "            return []\n",
    "\n",
    "aree_verdi = pd.read_csv('./data/raw/rilevazione_qualità_aria/aree_verdi.csv')\n",
    "nomi_aree_verdi = aree_verdi['PARCO'].unique()\n",
    "# Tutte i nomi dentro la lista devono avere la lettera iniziale grande.\n",
    "nomi_aree_verdi = [nome.title() for nome in nomi_aree_verdi]\n",
    "# convert the list to a string\n",
    "nomi_aree_verdi_string = ' '.join([f'\"{nome}\"' for nome in nomi_aree_verdi])\n",
    "print(nomi_aree_verdi_string)\n",
    "\n",
    "print(get_area_verde_uris(nomi_aree_verdi_string))\n",
    "\n",
    "# Iterare e aggiungere le triple RDF al grafo g per ogni URI ottenuto da Wikidata per le aree verdi\n",
    "for area_verde_name, area_verde_uri in zip(nomi_aree_verdi, get_area_verde_uris(nomi_aree_verdi_string)):\n",
    "    if area_verde_uri:\n",
    "        # Crea l'URIRef con l'URI trovato\n",
    "        area_verde_uri_ref = URIRef(area_verde_uri)\n",
    "        # Aggiungi le triple al grafo\n",
    "        g.add((area_verde_uri_ref, RDF.type, OWL.Thing))\n",
    "        g.add((area_verde_uri_ref, RDFS.label, Literal(area_verde_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import BNode\n",
    "\n",
    "\n",
    "popolazione_df = pd.read_csv('./data/processed/popolazione/popolazione_totale_2014_2021.csv')\n",
    "#ricoveri_df = pd.read_csv('./data/processed/ricoveri/ricoveri.csv')\n",
    "#aree_verdi_df = pd.read_csv('./data/processed/rilevazione_qualità_aria/aree_verdi.csv')\n",
    "stazioni_metereologiche_df = pd.read_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv')\n",
    "res = URIRef(urify(base_uri, \"Milano\"))\n",
    "\n",
    "haStazioniMetereologiche = BNode()\n",
    "\n",
    "def addSingleComuneTriple(popolazione_df):\n",
    "    #g.add([res, RDF.type, sso.Comune])\n",
    "    for index, row in popolazione_df.iterrows():\n",
    "        anno = row['Anno']\n",
    "        popolazione = row['Totale']\n",
    "        # Creazione di un Blank Node per ogni anno\n",
    "        popolazione_node = BNode()\n",
    "        g.add([res, sso.haPopolazione, popolazione_node])\n",
    "        g.add([popolazione_node, sso.anno, Literal(anno, datatype=XSD.gYear)])\n",
    "        g.add([popolazione_node, sso.totalePopolazione, Literal(popolazione, datatype=XSD.integer)])\n",
    "        # Aggiunta degli ID delle stazioni metereologiche\n",
    "    for index, row in stazioni_metereologiche_df.iterrows():\n",
    "        id_arpa = row['id_arpa']\n",
    "        stazione_node = BNode()\n",
    "        g.add([res, sso.haStazioneMetereologica, stazione_node])\n",
    "        g.add([stazione_node, sso.idArpa, Literal(id_arpa, datatype=XSD.integer)])\n",
    "\n",
    "\n",
    "# Applica la funzione\n",
    "addSingleComuneTriple(popolazione_df)\n",
    "\n",
    "# Stampa il grafo in formato Turtle\n",
    "print(g.serialize(format='turtle'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_stazioni = Graph()\n",
    "stazioni_metereologiche_df = pd.read_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv')\n",
    "base_uri_stazioni = \"http://www.sanitasicilia.it/resource/stazioni/\"\n",
    "\n",
    "def creaGrafoStazioni(g_stazioni, stazioni_df):\n",
    "    for index, row in stazioni_df.iterrows():\n",
    "        id_arpa = row['id_arpa']\n",
    "        nome = row['nome']\n",
    "        inquinanti = row['inquinanti']\n",
    "        latitudine = row['latitude']\n",
    "        longitudine = row['longitude']\n",
    "        stazione_uri = URIRef(f\"{base_uri_stazioni}{id_arpa}\")\n",
    "        # Aggiungi dettagli della stazione al grafo\n",
    "        g_stazioni.add([stazione_uri, RDF.type, sso.StazioneMetereologica])\n",
    "        g_stazioni.add([stazione_uri, sso.nome, Literal(nome, datatype=XSD.string)])\n",
    "        g_stazioni.add([stazione_uri, sso.inquinanti, Literal(inquinanti, datatype=XSD.string)])\n",
    "        g_stazioni.add([stazione_uri, sso.latitude, Literal(latitudine, datatype=XSD.decimal)])\n",
    "        g_stazioni.add([stazione_uri, sso.longitude, Literal(longitudine, datatype=XSD.decimal)])\n",
    "\n",
    "creaGrafoStazioni(g_stazioni, stazioni_metereologiche_df)\n",
    "print(g_stazioni.serialize(format='turtle'))\n",
    "\n",
    "for index, row in stazioni_metereologiche_df.iterrows():\n",
    "    id_arpa = row['id_arpa']\n",
    "    stazione_node_g = BNode()  # Nodo nel grafo g\n",
    "    stazione_uri_g_stazioni = URIRef(f\"{base_uri_stazioni}{id_arpa}\")  # Nodo nel grafo g_stazioni\n",
    "\n",
    "    # Aggiungi dati al grafo g\n",
    "    g.add([res, sso.haStazioneMetereologica, stazione_node_g])\n",
    "    g.add([stazione_node_g, sso.idArpa, Literal(id_arpa, datatype=XSD.integer)])\n",
    "\n",
    "    # Collega il nodo nel grafo g con il nodo corrispondente nel grafo g_stazioni\n",
    "    if (stazione_uri_g_stazioni, None, None) in g_stazioni:\n",
    "        g.add([stazione_node_g, OWL.sameAs, stazione_uri_g_stazioni])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi il file geojson delle aree verdi e crea un grafo RDF\n",
    "g_aree_verdi = Graph()\n",
    "\n",
    "base_uri_aree_verdi = \"http://www.sanitasicilia.it/resource/aree_verdi/\"\n",
    "\n",
    "with open('./data/processed/rilevazione_qualità_aria/aree_verdi.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for feature in data['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    longitude = geometry['coordinates'][0]\n",
    "    latitude = geometry['coordinates'][1]\n",
    "    nome = properties['PARCO']\n",
    "    codice_area = properties['CODICE_AREA']\n",
    "    zona = properties['ZONA']\n",
    "    id = properties['_id']\n",
    "    area_mq = properties['AREA_MQ']\n",
    "    perim_m = properties['PERIM_M']\n",
    "    area_verde_uri = URIRef(f\"{base_uri_aree_verdi}{id}\")\n",
    "    g_aree_verdi.add([area_verde_uri, RDF.type, sso.AreaVerde])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.nome, Literal(nome, datatype=XSD.string)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.latitude, Literal(latitude, datatype=XSD.decimal)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.longitude, Literal(longitude, datatype=XSD.decimal)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.codiceArea, Literal(codice_area, datatype=XSD.integer)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.zona, Literal(zona, datatype=XSD.string)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.area, Literal(area_mq, datatype=XSD.integer)])\n",
    "    g_aree_verdi.add([area_verde_uri, sso.perimetro, Literal(perim_m, datatype=XSD.integer)])\n",
    "        \n",
    "print(g_aree_verdi.serialize(format='turtle'))\n",
    "\n",
    "for area in data['features']:\n",
    "    id_av = area['properties']['_id']\n",
    "    area_node_g = BNode()  # Nodo nel grafo g\n",
    "    area_uri_g_aree_verdi = URIRef(f\"{base_uri_aree_verdi}{id_av}\")  # Nodo nel grafo g_aree_verdi\n",
    "\n",
    "    # Aggiungi dati al grafo g\n",
    "    g.add([res, sso.haAreaVerde, stazione_node_g])\n",
    "    g.add([area_node_g, sso.idAv, Literal(id_av, datatype=XSD.integer)])\n",
    "\n",
    "    # Collega il nodo nel grafo g con il nodo corrispondente nel grafo g_stazioni\n",
    "    if (area_uri_g_aree_verdi, None, None) in g_aree_verdi:\n",
    "        g.add([area_node_g, OWL.sameAs, area_uri_g_aree_verdi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, p, o in g.triples((None, OWL.sameAs, None)):\n",
    "    if (o, None, None) in g_stazioni:\n",
    "        print(f\"Il nodo {o} esiste in entrambi i grafi e è collegato.\")\n",
    "    if (o, None, None) in g_aree_verdi:\n",
    "        print(f\"Il nodo {o} esiste in entrambi i grafi e è collegato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **`5. - Visualizzazione dati`**\n",
    "\n",
    "#### **`5.1 - Creazione mappe con i GeoJson`**\n",
    "\n",
    "TODO\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **`4.2 - Conversione in RDF`**\n",
    "\n",
    "TODO\n",
    "\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
