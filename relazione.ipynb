{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><strong>Relazione di Open Data Management 2023-2024</strong></h1>\n",
    "<h3 align=\"center\"><strong><em>di Daniele Nicosia e Claudio Bellanti</em></strong></h3>\n",
    "<h5 align=\"center\"><em>Università degli Studi di Palermo - Facoltà di Informatica</em></h5>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "### [`1. - Traccia`](#1-traccia)\n",
    "### [`2. - Selezione dei dataset`](#2-selezione-dei-dataset)\n",
    "- ##### [`2.1 - Raccolta`](#21-raccolta)\n",
    "- ##### [`2.2 - Licenze`](#22-licenze)\n",
    "### [`3. - Elaborazione dei dataset`](#3-elaborazione-dei-dataset)\n",
    "- ##### [`3.1 - Pulizia e selezione dei dati rilevanti`](#31-pulizia-e-selezione-dei-dati-rilevanti)\n",
    "- ##### [`3.2 - Arricchimento`](#32-arricchimento)\n",
    "### [`4. - Finalizzazione dei dataset`](#4-finalizzazione-dei-dataset)\n",
    "- ##### [`4.1 - Ontologia`](#41-ontologia)\n",
    "- ##### [`4.2 - Conversione in RDF`](#42-conversione-in-rdf)\n",
    "### [`5. - Visualizzazione dati`](#5-visualizzazione-dati)\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1. - Traccia`**\n",
    "\n",
    "Utilizzando il linguaggio di programmazione Python, per lo sviluppo del progetto si devono innanzitutto rispettare i seguenti passi:\n",
    "\n",
    "- _Selezione dati_\n",
    "- _Elaborazione dati (data cleaning, definizione struttura omogenea)_\n",
    "- _Open Linked Data (creazione di uno strato semantico, ontologie, interlinking)_\n",
    "\n",
    "L'obiettivo della suddetta relazione è lo sviluppo di ???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2. - Selezione dei dataset`**\n",
    "\n",
    "#### **`2.1 - Raccolta`**\n",
    "\n",
    "Per lo sviluppo del progetto sono stati selezionati dei dataset i cui dati rappresentino gli eventi che si sono verificati nel Comune di Milano nel periodo 2014-2021. I dataset selezionati sono i seguenti:\n",
    "\n",
    "- [**`Parco veicoli circolanti per classe di inquinamento 2007-2022`**](https://dati.comune.milano.it/dataset/ds530-emissioni-inquinanti-standard-eu-2007-avanti)\n",
    "- [**`Ricoveri ordinari apparato respiratorio 2007-2021`**](https://dati.comune.milano.it/dataset/ds1053_ricoveri-ordinari-apparato-respiratorio)\n",
    "- [**`Stazioni di monitoraggio inquinanti`**](https://dati.comune.milano.it/dataset/ds484_stazioni_di_monitoraggio_inquinanti_atmosferici_dellarpa_sit)\n",
    "- [**`Ricostruzione della popolazione 2002-2019`**](https://demo.istat.it/app/?i=RIC&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2002 al 2019.\n",
    "- [**`Popolazione residente 2019-2023`**](https://demo.istat.it/app/?i=POS&l=it)\n",
    "  - Per questo dataset è stato necessario effettuare una personalizzazione tramite l'interfaccia di ISTAT. Il dataset risultante contiene la popolazione residente nel solo Comune di Milano dal 2020 al 2022.\n",
    "- [**`Standard di emissioni CO europei`**](https://en.wikipedia.org/wiki/European_emission_standards#Toxic_emission:_stages_and_legal_framework)\n",
    "  - Per questo dataset è stato necessario effettuare uno scraping del sito. Il dataset risultante contiene gli standard di emissioni CO europei dal 1992 al 2021.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`2.2 - Licenze`**\n",
    "\n",
    "Le licenze dei dataset selezionati sono le seguenti:\n",
    "\n",
    "- **`Parco veicoli circolanti per classe di inquinamento 2007-2022`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricoveri ordinari apparato respiratorio 2007-2021`**: Creative Commons Attribuzione-Condividi allo stesso modo 3.0 Italia ([CC BY-SA 3.0 IT](https://creativecommons.org/licenses/by-sa/3.0/it/))\n",
    "- **`Stazioni di monitoraggio inquinanti`**: Creative Commons Attribuzione 4.0 Internazionale ([CC BY 4.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Ricostruzione della popolazione 2002-2019`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://www.istat.it/it/note-legali))\n",
    "- **`Popolazione residente 2019-2023`**: Creative Commons Attribuzione 3.0 ([CC BY 3.0](https://creativecommons.org/licenses/by/4.0/))\n",
    "- **`Standard di emissioni CO europei`**: Creative Commons Attribuzione-ShareAlike 4.0 Internazionale ([CC BY-SA 4.0](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License))\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3. - Elaborazione dei dataset`**\n",
    "\n",
    "#### **`3.1 - Pulizia e selezione dei dati rilevanti`**\n",
    "\n",
    "Il primo passo effettuato è stato unire i diversi dataset della popolazione residente nel Comune di Milano, in un unico dataset.\n",
    "\n",
    "Si è notato che i dataset puri non erano tutti adattati allo stesso modo, infatti, si sono riscontrate due problematiche:\n",
    "\n",
    "- Encoding dei dataset differenti, a causa delle loro diversi origini\n",
    "- Errori vari dovuti a `;` o `,` non correttamente inseriti\n",
    "\n",
    "Inoltre, per il dataset delle emissioni è stato necessario effettuare uno scraping poichè non era disponibile in formato CSV. Per effettuare lo scraping è stato utilizzato il linguaggio di programmazione Python e la libreria `BeautifulSoup`. Lo script utilizzato è accessibile qui: [**`scraping.ipynb`**](dataset/emissioni/scraping.ipynb).\n",
    "\n",
    "##### *`Preparazione ambiente di sviluppo`*\n",
    "\n",
    "Prima di cominciare ad analizzare i dati dobbiamo importare le librerie necessarie al nostro scopo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset rilevazione della qualità dell'aria`*\n",
    "\n",
    "In questo estratto di codice, ci siamo concentrati sull'elaborazione di un insieme di dati complesso riguardante la qualità dell'aria, raccolto nel corso di diversi anni. Questi dati provengono da molteplici stazioni che, è importante notare, hanno capacità di rilevamento diverse. Non tutte le stazioni sono equipaggiate per misurare ogni tipo di inquinante, il che introduce una certa variabilità nei dati che stiamo trattando.\n",
    "\n",
    "Il primo passo significativo nel nostro processo è stato unire questi diversi file di dati, ognuno contenente frammenti del quadro generale della qualità dell'aria, in un unico, coerente DataFrame. Questo non solo ha semplificato le fasi successive dell'analisi, ma ha anche assicurato che ogni pezzo di informazione fosse contestualizzato all'interno del dataset più ampio.\n",
    "\n",
    "Una volta consolidati i dati, abbiamo affrontato la questione della uniformità temporale, assicurandoci che ogni data fosse in un formato standard e ordinando poi il dataset in ordine cronologico. Questo ordine temporale è essenziale per qualsiasi analisi storica o trend che potrebbe emergere da questi dati.\n",
    "\n",
    "Il problema successivo che abbiamo affrontato è stato quello dei valori mancanti, un'incidenza comune nei grandi set di dati, specialmente quando si tratta di rilevamenti ambientali su più anni. La strategia adottata qui è stata calcolare la mediana dei valori per ogni inquinante per ogni anno, una tecnica scelta per la sua resistenza agli outlier, ovvero quei valori che differiscono significativamente dalla norma e che potrebbero distorcere i risultati se non trattati correttamente.\n",
    "\n",
    "Utilizzando le mediane annuali, abbiamo potuto imputare valori ragionevoli nei casi in cui i dati erano mancanti, mantenendo l'integrità generale dei dati senza concedere un peso eccessivo a misurazioni anomale o inaccurate. Questo passaggio è essenziale per mantenere la validità delle nostre conclusioni finali.\n",
    "\n",
    "Dopo aver riempito questi vuoti, il nostro set di dati era quasi pronto per essere utilizzato in analisi future. Abbiamo rimosso alcune colonne superflue per rendere il dataset più snello e abbiamo gestito eventuali valori mancanti residui, assicurandoci che fossero chiaramente indicati.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('./data/raw/rilevazione_qualità_aria/*.csv')\n",
    "\n",
    "# Carica tutti i file CSV in un dizionario di DataFrame, specificando il separatore corretto\n",
    "dfs = {fp: pd.read_csv(fp, sep=';') for fp in file_paths}\n",
    "\n",
    "# Concatena tutti i DataFrame in un unico DataFrame\n",
    "air_quality = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Converti la colonna 'data' in datetime se non lo è già\n",
    "air_quality['data'] = pd.to_datetime(air_quality['data'])\n",
    "\n",
    "# Ordina il DataFrame in base alla colonna 'data' in ordine crescente\n",
    "air_quality.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Carica il file delle stazioni meteorologiche (adattalo al tuo percorso file)\n",
    "stazioni = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "# Estraiamo gli inquinanti che ogni stazione può misurare e creiamo un dizionario\n",
    "stazioni['inquinanti'] = stazioni['inquinanti'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])\n",
    "dict_stazioni_inquinanti = stazioni.set_index('id_amat')['inquinanti'].to_dict()\n",
    "\n",
    "# Crea un nuovo DataFrame pivotato\n",
    "air_quality_pivoted = pd.pivot_table(air_quality, values='valore', index=['stazione_id', 'data'], columns='inquinante').reset_index()\n",
    "air_quality_pivoted['data'] = pd.to_datetime(air_quality_pivoted['data'])\n",
    "air_quality_pivoted.sort_values('data', ascending=True, inplace=True)\n",
    "\n",
    "# Estrai l'anno da ogni data\n",
    "air_quality_pivoted['year'] = air_quality_pivoted['data'].dt.year\n",
    "\n",
    "# Calcolo delle mediane annuali e riempimento dei valori NaN\n",
    "annual_medians = {}\n",
    "for stazione_id, inquinanti in dict_stazioni_inquinanti.items():\n",
    "    for inquinante in inquinanti:\n",
    "        if inquinante in air_quality_pivoted.columns:\n",
    "            data_grouped = air_quality_pivoted[air_quality_pivoted['stazione_id'] == stazione_id].groupby('year')\n",
    "            medians_by_year = data_grouped[inquinante].median()\n",
    "\n",
    "            for year, median in medians_by_year.items():\n",
    "                annual_medians[(stazione_id, inquinante, year)] = median\n",
    "\n",
    "for key, median in annual_medians.items():\n",
    "    stazione_id, inquinante, year = key\n",
    "    mask = (air_quality_pivoted['stazione_id'] == stazione_id) & \\\n",
    "           (air_quality_pivoted['year'] == year) & \\\n",
    "           (air_quality_pivoted[inquinante].isna())\n",
    "    \n",
    "    air_quality_pivoted.loc[mask, inquinante] = median\n",
    "\n",
    "# Rimozione della colonna 'year' dopo aver completato le operazioni di imputazione\n",
    "air_quality_pivoted = air_quality_pivoted.drop(columns='year')\n",
    "air_quality_pivoted = air_quality_pivoted.fillna(\"N.D.\")\n",
    "\n",
    "# Salvataggio del DataFrame aggiornato\n",
    "air_quality_pivoted.to_csv('./data/processed/rilevazione_qualità_aria/air_quality_pivoted.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson stazioni metereologiche`*\n",
    "\n",
    "Il codice qui presentato si occupa essenzialmente di trasformare un elenco di stazioni meteorologiche, con varie informazioni associate, in un formato facilmente utilizzabile per applicazioni geospaziali. Inizialmente, vengono eliminate alcune informazioni non necessarie, concentrando l'attenzione su dati più pertinenti, come la posizione geografica di ogni stazione.\n",
    "\n",
    "Una parte cruciale di questo processo riguarda la manipolazione delle coordinate geografiche. Dal momento che le posizioni sono state fornite come testo, il codice le converte in formato GeoJSON; uno standard specifico per i dati geografici.\n",
    "Questo formato è particolarmente utile perché è ampiamente riconosciuto e utilizzato in molte applicazioni di mappatura, rendendo i dati facilmente accessibili e utilizzabili.\n",
    "Alla fine di questa procedura avremo la possibilità, attraverso librerie e strumenti online, di ottenere una mappa con i nostri specifici \"Point\" e i relativi metadati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioni_metereologiche = pd.read_csv('./data/raw/rilevazione_qualità_aria/stazioni_mereologiche.csv')\n",
    "\n",
    "stazioni_metereologiche.drop(columns=['inizio_operativita', 'fine_operativita','LONG_X_4326','LAT_Y_4326'], inplace=True)\n",
    "\n",
    "    # Estrai le coordinate dalla colonna \"Location\"\n",
    "stazioni_metereologiche['coordinates'] = stazioni_metereologiche['Location'].str.strip('()').str.split(', ')\n",
    "stazioni_metereologiche['latitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[0]))\n",
    "stazioni_metereologiche['longitude'] = stazioni_metereologiche['coordinates'].apply(lambda x: float(x[1]))\n",
    "\n",
    "    # Crea la struttura base GeoJSON\n",
    "geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in stazioni_metereologiche.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['_id', 'Location', 'coordinates', 'latitude', 'longitude']).to_dict()\n",
    "    }\n",
    "    geojson[\"features\"].append(feature)\n",
    "\n",
    "    # Salva il GeoJSON in un file\n",
    "with open(\"./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.geojson\", 'w') as f:\n",
    "    json.dump(geojson, f)\n",
    "\n",
    "stazioni_metereologiche.to_csv('./data/processed/rilevazione_qualità_aria/stazioni_metereologiche.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset GeoJson aree verdi`*\n",
    "\n",
    "Al fine di effettuare una analisi completa, abbiamo optato di inserire all'interno dell'analisi anche le aree verdi di Milano.\n",
    "In questo caso abbiamo prelevato i dati forniti in CSV dal comune di Milano e ne abbiamo realizzato un Geo JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aree_verdi = pd.read_csv('./data/raw/rilevazione_qualità_aria/aree_verdi.csv')\n",
    "if 'Location' in aree_verdi.columns:\n",
    "    aree_verdi['coordinates'] = aree_verdi['Location'].str.strip('()').str.split(', ')\n",
    "    aree_verdi['latitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[0]))\n",
    "    aree_verdi['longitude'] = aree_verdi['coordinates'].apply(lambda x: float(x[1]))\n",
    "else:\n",
    "    # Se non esiste una colonna 'Location', le coordinate potrebbero essere presenti come colonne separate\n",
    "    # Assicurati che le colonne 'latitude' e 'longitude' esistano nel tuo file CSV\n",
    "    aree_verdi['latitude'] = aree_verdi['latitude'].apply(float)  # Converti in float se già non lo sono\n",
    "    aree_verdi['longitude'] = aree_verdi['longitude'].apply(float)  # Converti in float se già non lo sono\n",
    "\n",
    "# Crea la struttura base GeoJSON\n",
    "geojson_aree_verdi = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Popola GeoJSON con le features\n",
    "for index, row in aree_verdi.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['longitude'], row['latitude']]\n",
    "        },\n",
    "        \"properties\": row.drop(['Location', 'coordinates', 'latitude', 'longitude']).to_dict()  # escludi le colonne non necessarie\n",
    "    }\n",
    "    geojson_aree_verdi[\"features\"].append(feature)\n",
    "\n",
    "# Salva il GeoJSON in un file\n",
    "output_file = \"./data/processed/rilevazione_qualità_aria/aree_verdi.geojson\"  # Modifica il percorso del file secondo necessità\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(geojson_aree_verdi, f)\n",
    "\n",
    "# Se necessario, salva anche il DataFrame modificato in un nuovo file CSV\n",
    "#aree_verdi.to_csv('/mnt/data/aree_verdi_processed.csv', index=False, na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset emissioni veicoli`*\n",
    "Il dataset in questione sulle emissioni dei veicoli basata sulla classificazione EURO è stato ottenuto effettuando web scraping.\n",
    "Con esso siamo riusciti a prelevare da wikipedia diverse tabelle dedicate a diversi tipi di veicolo, nonostante ciò il loro contenuto risultava particolarmente sporco e non completo.\n",
    "Quindi, prima di poter utilizzare questi dati abbiamo dovuto effettuare un lavoro di pulizia manuale, in quanto automatizzare l'operazione risultava particolarmente onerosa e non la scelta ideale.\n",
    "Per alcuni dataset è stato cambiato il delimitatore in quanto era utilizzato il punto e virgola, per una maggiore coerenza è stata sostituita con la virgola.\n",
    "Sono stati inoltre standardizzati i valori della colonna \"Tier\" in quanto erano presenti in alcuni CSV i numeri romani piuttosto che quelli arabi.\n",
    "\n",
    "Todo:\n",
    "Basta confrontare il codice con i vecchi csv per capire le modifiche manuali e quelle automatizzate, in teoria ho scritto tutto ma non si sa mai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/emissioni/'\n",
    "percorso_output = './data/processed/emissioni/'\n",
    "\n",
    "# Assicurati che le cartelle esistano, se no, creale\n",
    "os.makedirs(percorso_output, exist_ok=True)\n",
    "\n",
    "# Trova tutti i file CSV nel percorso di input\n",
    "files = glob.glob(os.path.join(percorso_input, '*.csv'))\n",
    "\n",
    "for file in files:\n",
    "    file_name = os.path.basename(file)\n",
    "    if file_name == 'standards_for_motor_cycles.csv':\n",
    "        df = pd.read_csv(file, delimiter=',')\n",
    "        if 'Standard' in df.columns:\n",
    "            df.rename(columns={'Standard': 'Tier'}, inplace=True)\n",
    "        df.to_csv(os.path.join(percorso_output, file_name), index=False, na_rep='NaN')\n",
    "        continue  # Salta questo file\n",
    "    # Leggi il CSV corrente\n",
    "    df = pd.read_csv(file, delimiter=';')\n",
    "\n",
    "    # Preparare il percorso del file di output\n",
    "    nome_file = os.path.basename(file)  # Estrae il nome del file dal percorso\n",
    "    file_di_output = os.path.join(percorso_output, nome_file)\n",
    "\n",
    "    #Droppa le colonne che non ci servono\n",
    "    if 'Date (type approval)' in df.columns:\n",
    "        df.drop(columns=['Date (type approval)'], axis=0, inplace=True)\n",
    "    #Cambiamo il nome delle colonne strane xd\n",
    "    if 'Date (first registration)' in df.columns:\n",
    "        df.rename(columns={'Date (first registration)': 'Date'}, inplace=True)\n",
    "    if 'Standard' in df.columns:\n",
    "        df.rename(columns={'Standard': 'Tier'}, inplace=True)\n",
    "\n",
    "    # Elimino tutto ciò che non è un numero in tutte le righe della colonna Date\n",
    "    df['Date'] = df['Date'].apply(lambda x: re.sub('[^0-9]', '', str(x)))\n",
    "\n",
    "    df.to_csv(file_di_output, index=False, sep=',', na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset veicoli per classe di inquinamento`*\n",
    "\n",
    "Il seguente dataset è stato ottenuto rimuovendo le colonne degli anni che non ci interessava analizzare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/veicoli/categorie_euro_2007_2022.csv'\n",
    "percorso_output = './data/processed/veicoli/categorie_euro_2014_2021.csv'\n",
    "\n",
    "veicoli = pd.read_csv(percorso_input, sep=';')\n",
    "\n",
    "veicoli = veicoli[veicoli['ANNO'].between(2014, 2021)]\n",
    "\n",
    "veicoli = veicoli.reset_index(drop=True)\n",
    "\n",
    "veicoli = veicoli.fillna(0)\n",
    "\n",
    "veicoli.columns = veicoli.columns.str.title()\n",
    "\n",
    "veicoli.to_csv(percorso_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *`Creazione dataset popolazione`*\n",
    "\n",
    "Il dataset in questione è stato ottenuto, innanzitutto, rimuovendo le colonne degli anni che non ci interessava analizzare. Successivamente, è stato necessario unire dei dataset poichè sono stati cambiati i metadati e la loro disposizione negli anni successivi al 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percorso_input = './data/raw/popolazione/'\n",
    "percorso_output = './data/processed/popolazione/'\n",
    "\n",
    "popolazione = pd.read_csv(percorso_input + 'ricostruzione_popolazione_2002_2019.csv')\n",
    "\n",
    "anni = np.arange(2014, 2021)\n",
    "for col in popolazione.columns:\n",
    "    try:\n",
    "        year = int(col)\n",
    "        if year not in anni:\n",
    "            popolazione = popolazione.drop(columns=col)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "file_list = os.listdir(percorso_input)\n",
    "popolazione_files = [f for f in file_list if f.startswith('popolazione')]\n",
    "\n",
    "prev_age = None\n",
    "curr_row = None\n",
    "\n",
    "for filename in popolazione_files:\n",
    "    new_pop = pd.read_csv(os.path.join(percorso_input, filename))\n",
    "\n",
    "    year = filename[-8:-4]\n",
    "    popolazione[year] = 0\n",
    "\n",
    "    for index, row in popolazione.iterrows():\n",
    "        age = row['Età']\n",
    "        if age != prev_age:\n",
    "            curr_row = new_pop.loc[new_pop['Età'] == age].iloc[0]\n",
    "            prev_age = age\n",
    "\n",
    "        if popolazione.loc[index, 'Sesso'] == 'Maschi':\n",
    "            popolazione.loc[index, year] = curr_row['Totale maschi']\n",
    "        elif popolazione.loc[index, 'Sesso'] == 'Femmine':\n",
    "            popolazione.loc[index, year] = curr_row['Totale femmine']\n",
    "        else:\n",
    "            popolazione.loc[index, year] = curr_row['Totale']\n",
    "\n",
    "popolazione.to_csv(percorso_output + 'popolazione_2014_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`3.2 - Arricchimento`**\n",
    "\n",
    "Successivamente alla fase di pulizia e selezione, si è introdotta una fase di arricchimento dei dataset.\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4. - Finalizzazione dei dataset`**\n",
    "\n",
    "#### **`4.1 - Ontologia`**\n",
    "\n",
    "???\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **`4.2 - Conversione in RDF`**\n",
    "\n",
    "???\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **`5. - Visualizzazione dati`**\n",
    "\n",
    "???\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
